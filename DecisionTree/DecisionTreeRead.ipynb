{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Decision Tree**\n",
    "\n",
    "A decision tree is a decision  tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs. It is one way to display an algorithm that only contains conditional control statements.\n",
    "\n",
    "**2. Terms Used in Decision Tree**\n",
    "\n",
    "***a)Entropy:*** \n",
    "\n",
    "To calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero             and if the sample is an equally divided it has entropy of one.\n",
    "            \n",
    "            Min val:0\n",
    "            Max val:1\n",
    "            Best val:0\n",
    "            Formula: E(s) = Sum(-p(i))log(p(i))  { Where ‘Pi’ is simply the frequentist probability of an                                                            element/class ‘i’ in our data.\n",
    "            \n",
    "***b)Information Gain***: \n",
    "\n",
    "Information gain is the amount of information that's gained by knowing the value of the                            attribute.How much entropy removed.\n",
    "            \n",
    "            Min val:0\n",
    "            Max val:1\n",
    "            Best val:1\n",
    "            Formula: 1 - E(s)\n",
    "            Information Gain = Entropy before splitting - Entropy after splitting\n",
    "            higher Information Gain = more Entropy removed\n",
    "   \n",
    "***c)Gini Impurity***: \n",
    "\n",
    "Gini Impurity is a measurement of the likelihood of an incorrect classification of a new                           instance of a random variable\n",
    "i) It works on categorical variables, provides outcomes either be “successful” or “failure” and                       hence conducts binary splitting only.\n",
    "ii) used in classification\n",
    "\n",
    "            Min val:0\n",
    "            Max val:1   (Randomly classified)\n",
    "            Best val:0\n",
    "            Formula: 1 - sum(p(i)**2)\n",
    "            \n",
    "**3. What Are the Basic Assumption**\n",
    "\n",
    "No Assumption\n",
    "\n",
    "**4. Does Decision Tree require Feature Scaling**\n",
    "\n",
    "Do not require feature scaling to be performed as they are not sensitive to the the variance in the data. \n",
    "\n",
    "**5. Impact of Outliers Of Decision Tree**\n",
    "\n",
    "Most likely outliers will have a negligible effect because the nodes are determined based on the sample proportions in each split region (and not on their absolute values).\n",
    "\n",
    "**6. Impact of mising values on Decision Tree**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**7. Impact of Multicollinearity in features**\n",
    "\n",
    "Decision Trees are not affected by multicollinearity in features\n",
    "\n",
    "**8. Cost function in Decision Tree**\n",
    "\n",
    "1. ***Regression:*** The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle.\n",
    "2. ***Classification:*** The Gini cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is.\n",
    "\n",
    "\n",
    "**9. Advantages of Decision Tree**\n",
    "\n",
    "1. Clear Visualization: The algorithm is simple to understand, interpret and visualize as the idea is mostly used in our daily lives. Output of a Decision Tree can be easily interpreted by humans.\n",
    "\n",
    "2. Simple and easy to understand: Decision Tree looks like simple if-else statements which are very easy to understand.\n",
    "\n",
    "3. Decision Tree can be used for both classification and regression problems.\n",
    "\n",
    "4. Decision Tree can handle both continuous and categorical variables.\n",
    "\n",
    "5. No feature scaling required: No feature scaling (standardization and normalization) required in case of Decision Tree as it uses rule based approach instead of distance calculation.When you need less effort for data preprocessing you can choose Decision Tree. Decision tree does not require normalization, scaling of data.\n",
    "\n",
    "6. Handles non-linear parameters efficiently: Non linear parameters don't affect the performance of a Decision Tree unlike curve based algorithms. So, if there is high non-linearity between the independent variables, Decision Trees may outperform as compared to other curve based algorithms.\n",
    "\n",
    "7. Decision Tree can automatically handle missing values.\n",
    "\n",
    "8. Decision Tree is usually robust to outliers and can handle them automatically.\n",
    "\n",
    "9. Less Training Period: Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest. \n",
    "\n",
    "11. Interpretability: The main advantage of the decision tree is its interpretability. If you want your model to explain which features are more important and information about at which value of the feature we are splitting for patterns, the decision tree is useful.\n",
    "\n",
    "**10. Disadvantages of Decision Tree**\n",
    "\n",
    "1. Overfitting: This is the main problem of the Decision Tree. It generally leads to overfitting of the data which ultimately leads to wrong predictions. In order to fit the data (even noisy data), it keeps generating new nodes and ultimately the tree becomes too complex to interpret. In this way, it loses its generalization capabilities. It performs very well on the trained data but starts making a lot of mistakes on the unseen data.\n",
    "\n",
    "2. High variance: As mentioned in point 1, Decision Tree generally leads to the overfitting of data. Due to the overfitting, there are very high chances of high variance in the output which leads to many errors in the final estimation and shows high inaccuracy in the results. In order to achieve zero bias (overfitting), it leads to high variance. \n",
    "\n",
    "***Decision Tree Low Bias And High Variance- Overfitting***\n",
    "\n",
    "a) More Bias = error from the model being more simpler (does not fit the data very well)\n",
    "b) More Variance = error from the model being more complex (fits the data too well, and learns the noise in addition to the inherent patterns in the data)\n",
    "if the number of levels is too high i.e a complicated decision tree, the model tends to overfit.\n",
    "\n",
    "Intuitively, it can be understood in this way. When there are too many decision nodes to go through before arriving at the result i.e number of nodes to traverse before reaching the leaf nodes is high, the conditions that you are checking against becomes multiplicative. That is, the computation becomes (condition 1)&&(condition 2)&&(condition 3)&&(condition 4)&&(condition5).\n",
    "Only if all the conditions are satisfied, a decision is reached. As you can see, this will work very well for the training set as you are continuously narrowing down on the data. The tree becomes highly tuned to the data present in the training set.\n",
    "But when a new data point is fed, even if one of the parameters deviates slightly, the condition will not be met and it will take the wrong branch.\n",
    "\n",
    "\n",
    "3. Unstable: Adding a new data point can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated. \n",
    "\n",
    "4. Affected by noise: Little bit of noise can make it unstable which leads to wrong predictions.\n",
    "\n",
    "5. Not suitable for large datasets: If data size is large, then one single tree may grow complex and lead to overfitting. So in this case, we should use Random Forest instead of a single Decision Tree.\n",
    "\n",
    "\n",
    "\n",
    "**11. Performance Metrics**\n",
    "\n",
    "**Classification**\n",
    "\n",
    "1. Confusion Matrix\n",
    "2. Precision,Recall, F1 score\n",
    "\n",
    "**Regression**\n",
    "\n",
    "1. R2,Adjusted R2\n",
    "2. MSE,RMSE,MAE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
